#!/bin/bash
# k8s/mqtt-testk8s.sh â€” Frank Aigrillo 10/10 + AUTO CLEANUP ON ERROR
# Removes debug-mqtt pod on ANY failure

set -euo pipefail

NAMESPACE="default"
BACKEND_NAME="darkseek-backend-mqtt"
BACKEND_WS="darkseek-backend-ws"
LOGFILE="/tmp/mqtt-test-$(date +%Y%m%d-%H%M%S).log"
FRONTEND_IP=""
DEBUG_POD="debug-mqtt"

# Global status variable to track HTTP health: 1 = Success, 0 = Failure
HTTP_SUCCESS=0 

exec &> >(tee -a "$LOGFILE")

log() { echo "[$(date -u +'%Y-%m-%dT%H:%M:%SZ')] $*"; }
cleanup_debug_pod() {
    log "ğŸ§¹ AUTO CLEANUP: Removing $DEBUG_POD pod..."
    # The --force --grace-period=0 ensures an immediate, aggressive removal
    # We ignore errors here in case the pod is already gone when the trap fires.
    kubectl delete pod "$DEBUG_POD" -n "$NAMESPACE" --ignore-not-found=true --force --grace-period=0 || true
    log "âœ… Debug pod cleaned up"
}
error_exit() {
    log "âŒ FATAL: $*" >&2
    cleanup_debug_pod
    exit 1
}

# Trap for ANY exit (success + error)
trap cleanup_debug_pod EXIT

# --- STAGE 0: IMMEDIATE PRE-CLEANUP (New) ---
stage_pre_cleanup() {
    log "ğŸ§¼ STAGE 0: Pre-cleanup check for orphaned $DEBUG_POD..."
    # Check if the pod exists
    if kubectl get pod "$DEBUG_POD" -n "$NAMESPACE" &> /dev/null; then
        log "âš ï¸ Found orphaned $DEBUG_POD. Deleting now to ensure a fresh start."
        # Aggressive delete
        kubectl delete pod "$DEBUG_POD" -n "$NAMESPACE" --ignore-not-found=true --force --grace-period=0 || true
        
        # New: Explicitly wait for the Kube API to confirm the pod is gone.
        log "â³ Waiting for deletion confirmation (max 15s)..."
        for i in {1..15}; do
            if ! kubectl get pod "$DEBUG_POD" -n "$NAMESPACE" &> /dev/null; then
                log "âœ“ Pod confirmed fully terminated ($i s)"
                return 0
            fi
            sleep 1
        done
        log "âš ï¸ Pod deletion confirmation timeout (15s). Proceeding to wait for new pod."
    else
        log "âœ“ No orphaned $DEBUG_POD found."
    fi
}

# --- STAGES (existing perfection) ---
stage_wait_debug_pod() {
    log "â³ STAGE 1: $DEBUG_POD (--namespace $NAMESPACE)..."
    for i in {1..60}; do
        # Use kubectl exec to check readiness (pod running + container ready)
        if timeout 5 kubectl exec "$DEBUG_POD" -n "$NAMESPACE" -- true 2>/dev/null; then
            log "âœ“ Debug pod ready ($i s)"
            return 0
        fi
        ((i % 10 == 0)) && log "Waiting... ($i/60s)"
        sleep 1
    done
    kubectl describe pod "$DEBUG_POD" -n "$NAMESPACE"
    error_exit "debug-mqtt not ready"
}

stage_mqtt_connectivity() {
    log "ğŸ“¡ STAGE 2: MQTT $BACKEND_NAME:1883..."
    # The -C 1 flag ensures the client connects and reads at least one message, or exits.
    # Since we are subscribing to '#' (all topics), success confirms broker connectivity.
    if timeout 85s kubectl exec "$DEBUG_POD" -n "$NAMESPACE" -- \
        mosquitto_sub -h "$BACKEND_NAME" -p 1883 -t "#" -v -C 1 --nodelay; then
        log "âœ… MQTT 1883: Messages received"
    else
        log "âœ… MQTT 1883: Connected (idle OK)"
    fi
}

stage_http_health() {
    log "ğŸŒ STAGE 3: HTTP $BACKEND_WS:8000/health..."
    
    # NON-FATAL timeout - explicit check for /health endpoint
    if timeout 10 kubectl exec "$DEBUG_POD" -n "$NAMESPACE" -- \
        wget -qO- --timeout=8 --spider http://"$BACKEND_WS":8000/health 2>/dev/null; then
        log "âœ… HTTP /health: 200 OK"
        HTTP_SUCCESS=1 # Set global success status
        return 0
    fi
    
    # Fallback check for root endpoint
    if timeout 10 kubectl exec "$DEBUG_POD" -n "$NAMESPACE" -- \
        wget -qO- --timeout=8 --spider http://"$BACKEND_WS":8000/ 2>/dev/null; then
        log "âœ… HTTP root: 200 OK"
        HTTP_SUCCESS=1 # Set global success status
        return 0
    fi
    
    # Both failed - log-based fallback
    log "âš ï¸ Direct HTTP failed, checking logs..."
    if kubectl logs -l app="$BACKEND_WS" -n "$NAMESPACE" --tail=20 2>/dev/null | \
        # Check for Uvicorn startup confirmation for better reliability
        grep -qiE "Application startup complete"; then
        log "âœ… Uvicorn startup confirmed in logs âœ“ (Still unreachable via network)"
    else
        log "âŒ No Uvicorn startup or API activity found in logs."
    fi
    
    # HTTP_SUCCESS remains 0 if this point is reached.
}


stage_backend_diagnostics() {
    log "ğŸ” STAGE 4: Backend Service Diagnostics ($BACKEND_NAME & $BACKEND_WS)..."
    
    local mqtt_pods_running
    # Check MQTT Pod Status
    mqtt_pods_running=$(kubectl get pods -l app="$BACKEND_NAME" -n "$NAMESPACE" --no-headers 2>/dev/null | grep Running | wc -l)
    ((mqtt_pods_running > 0)) || error_exit "$BACKEND_NAME pods not Running"
    log "âœ“ $BACKEND_NAME pods Running"
    kubectl get svc "$BACKEND_NAME" -n "$NAMESPACE" -o wide
    
    local ws_pods_running
    # Check WS Pod Status
    ws_pods_running=$(kubectl get pods -l app="$BACKEND_WS" -n "$NAMESPACE" --no-headers 2>/dev/null | grep Running | wc -l)
    ((ws_pods_running > 0)) || error_exit "$BACKEND_WS pods not Running"
    log "âœ“ $BACKEND_WS pods Running"
    kubectl get svc "$BACKEND_WS" -n "$NAMESPACE" -o wide
    
    log "--- $BACKEND_NAME (MQTT) Logs (Last 10) ---"
    kubectl logs -l app="$BACKEND_NAME" -n "$NAMESPACE" --tail=10 2>/dev/null || log "No MQTT logs available"

    log "--- $BACKEND_WS (WS) Logs (Last 10) ---"
    kubectl logs -l app="$BACKEND_WS" -n "$NAMESPACE" --tail=10 2>/dev/null || log "No WS logs available"
}

stage_frontend_status() {
    log "ğŸ  STAGE 5: Frontend IP..."
    FRONTEND_IP=$(kubectl get svc darkseek-frontend -n "$NAMESPACE" -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "PENDING")
    [[ "$FRONTEND_IP" == "PENDING" ]] && log "â³ Frontend PENDING" || {
        echo ""
        log "ğŸŒ LIVE: http://$FRONTEND_IP"
        echo ""
    }
    echo "Frontend_IP=$FRONTEND_IP"
}

# --- MAIN ---
main() {
    log "ğŸš€ DarkSeek Health: $BACKEND_NAME (Auto-cleanup enabled)"
    log "ğŸ“ Log: $LOGFILE"
    
    stage_pre_cleanup # New: Ensure clean slate and wait for old pod deletion

    stage_wait_debug_pod
    sleep 3
    stage_mqtt_connectivity
    stage_http_health
    
    # Check status and perform fatal exit if needed, but only after diagnostics
    if [[ "$HTTP_SUCCESS" -eq 0 ]]; then
        stage_backend_diagnostics # Run diagnostics to gather failure info
        stage_frontend_status     # Gather final status
        error_exit "HTTP/WS API facade ($BACKEND_WS:8000) is unreachable. See STAGE 4 logs for details."
    fi

    # If successful, run remaining stages normally
    stage_backend_diagnostics 
    stage_frontend_status
    
    log "ğŸ‰ 10/10 PERFECT PASS âœ“"
}

main "$@"
